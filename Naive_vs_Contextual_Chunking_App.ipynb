{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohpark/Files/blob/main/Naive_vs_Contextual_Chunking_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compares Naive Chunking to Contextual Chunking for RAG Retrieval Comparisons\n",
        "\n",
        "This app uses a Gradio interface that allows you to add your OpenAI and Pinecone credentials, as well as provide a PDF document. To work properly, the PDF must have text/ocr embedded already.\n",
        "\n",
        "The Document is parsed two ways: using Langchain's RecursiveCharacterTextSplitter using user-configurable chunk sizes and the default delimiters, and via llmsherpa, with a bit of post-processing to establish a minimum and maximum total chunk size.\n",
        "\n",
        "Each set of chunks is upserted to your Pinecone database using OpenAI's text-ada-002 embedding model via langchain. The first tab will display the overall chunking results once the upsert is complete.\n",
        "\n",
        "NOTE: This notebook will OVERWRITE the designated Namespaces in your Pinecone Index. You can change the default Namespace values if you desire, but *DO NOT* provide a Namespace you would like to otherwise preserve. This is simulating an application where temporary vectors are created for document RAG querying, then discarded.\n",
        "\n",
        "The default LLM in this notebook is GPT-4. If you have the base rate limit of 10,000 tokens/minute, then you may need to add some rate throttling, or downgrade to gpt-3.5-turbo-16k. Sorry for any inconvenience. Search for 'low-rate-limit' in this notebook to find the relevant function."
      ],
      "metadata": {
        "id": "eI7vevDoNV6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu135SehuR0C"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio langchain unstructured pdf2image openai tiktoken pdfminer.six uuid pinecone-client llmsherpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KOFJpQ9purom"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "from llmsherpa.readers import LayoutPDFReader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore import document\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "from google.colab import drive\n",
        "import tiktoken\n",
        "import pdfminer\n",
        "import pinecone\n",
        "import json, uuid, string, datetime, os, time, math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkeBgno_dw8p"
      },
      "source": [
        "#RAG Retrieval and LLM Calls\n",
        "\n",
        "Lanchain, mainly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9hbna2Wpdxdc"
      },
      "outputs": [],
      "source": [
        "def rag_qa(user_settings, my_question, my_namespace):\n",
        "  # for low-rate-limit gpt-4 users, you may need to add rate limiting somewhere in this function, or use gpt-3.5-turbo-16k.\n",
        "  messages = []\n",
        "  chat = ChatOpenAI(\n",
        "      openai_api_key = user_settings['openai_key'],\n",
        "      #model='gpt-3.5-turbo-16k',\n",
        "      model = 'gpt-4',\n",
        "      max_tokens=1024,\n",
        "      temperature=0\n",
        "  )\n",
        "  my_vectors = get_vectors(user_settings, my_question, my_namespace)\n",
        "  system_question = f\"\"\"Answer the Question based only on the facts and reasoning found in the Context. You are an analyst providing a detailed legal analysis to the question incorporating all of the information found in the Context.\n",
        "If you cannot find information relevant to Question in the Context, say that the answer wasn't found.\n",
        "\n",
        "Question: {my_question}\"\"\"\n",
        "\n",
        "  context_prompt = SystemMessage(content=my_vectors)\n",
        "  system_prompt = SystemMessage(content=system_question)\n",
        "  messages.append(context_prompt)\n",
        "  messages.append(system_prompt)\n",
        "  answer = chat(messages)\n",
        "  return answer.content, my_vectors\n",
        "\n",
        "def get_vectors(user_settings, my_question, my_namespace):\n",
        "  embeddings = OpenAIEmbeddings(openai_api_key=user_settings['openai_key'])\n",
        "  pinecone.init(\n",
        "      api_key = user_settings['pinecone_key'],\n",
        "      environment=user_settings['pinecone_environment']\n",
        "  )\n",
        "  index = pinecone.Index(user_settings['pinecone_index'])\n",
        "  vectorstore = Pinecone(index, embeddings, \"text\")\n",
        "  my_retrieval = vectorstore.similarity_search(my_question, k=user_settings['top_k'], namespace=my_namespace)\n",
        "  page_contents = [doc.page_content for doc in my_retrieval]\n",
        "  rag_contents = \"\"\n",
        "  for i, page in enumerate(page_contents, 1):\n",
        "    rag_contents += f\"Document {i}: \" + page + \"\\n\\n\"\n",
        "  return rag_contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbcs0Sbb0ajW"
      },
      "source": [
        "##Vector DB Bits\n",
        "\n",
        "Langchain, Pinecone, OpenAI (text embeddings), LLM Sherpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZX6zYe630eKM"
      },
      "outputs": [],
      "source": [
        "def tiktoken_len(text, base='cl100k_base'):\n",
        "  tokenizer = tiktoken.get_encoding(base)\n",
        "  tokens = tokenizer.encode(\n",
        "      text,\n",
        "      disallowed_special=()\n",
        "  )\n",
        "  return len(tokens)\n",
        "\n",
        "def upsert_vectors(user_settings, chunks, namespace):\n",
        "  embeddings = OpenAIEmbeddings(openai_api_key=user_settings['openai_key'])\n",
        "  pinecone.init(\n",
        "      api_key = user_settings['pinecone_key'],\n",
        "      environment=user_settings['pinecone_environment']\n",
        "  )\n",
        "  index = pinecone.Index(user_settings['pinecone_index'])\n",
        "  namespace_clear = check_namespace(user_settings, namespace)\n",
        "\n",
        "  upsert = Pinecone.from_texts(chunks, embeddings, index_name=user_settings['pinecone_index'], namespace=namespace)\n",
        "\n",
        "  chunks_list = \"\"\n",
        "  for i, chunk in enumerate(chunks, 1):\n",
        "    chunks_list += f\"Chunk #{i}: \" + \"\\n\\n\" + chunk + \"\\n\\n\"\n",
        "\n",
        "  return chunks_list\n",
        "\n",
        "#def contextual_vectors(user_settings, contextual_chunks)\n",
        "\n",
        "def check_namespace(user_settings, test_namespace):\n",
        "  pinecone.init(\n",
        "      api_key=user_settings['pinecone_key'],\n",
        "      environment=user_settings['pinecone_environment']\n",
        "      )\n",
        "  index = pinecone.Index(user_settings['pinecone_index'])\n",
        "  index_stats = index.describe_index_stats()\n",
        "  if test_namespace in index_stats['namespaces']:\n",
        "    delete_response = index.delete(deleteAll='true', namespace=test_namespace)\n",
        "    index_stats = index.describe_index_stats()\n",
        "    if test_namespace in index_stats['namespaces']:\n",
        "      raise Exception(f\"Failed to delete namespace: {test_namespace}\")\n",
        "    return True\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uO9XrKuKLFwX"
      },
      "outputs": [],
      "source": [
        "# @title Chunking Functions\n",
        "\n",
        "def doc_upload(document):\n",
        "    loader = UnstructuredPDFLoader(document.name)\n",
        "    doc_text = loader.load()\n",
        "    doc_content = doc_text[0].page_content[:]\n",
        "    doc_tokens = tiktoken_len(doc_content)\n",
        "\n",
        "    return document, doc_content, doc_tokens\n",
        "\n",
        "def text_splitter(doc, max_tokens, overlap_tokens=0):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = int(max_tokens), #chunk_s, # number of units per chunk\n",
        "      chunk_overlap = int(overlap_tokens), # number of units of overlap\n",
        "      length_function = tiktoken_len, #use tokens as chunking unit instead of characters.\n",
        "      separators=['\\n\\n', '\\n'] # our chosen operators for separating\n",
        "      )\n",
        "  texts = text_splitter.split_text(doc)\n",
        "  return texts\n",
        "\n",
        "def combine_chunks(chunks, min_tokens):\n",
        "    combined_chunks = []\n",
        "    buffer_chunk = \"\"\n",
        "    buffer_length = 0\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_text = chunk.to_context_text()  # Extract text representation of the chunk\n",
        "\n",
        "        # Add newline if buffer_chunk already has content\n",
        "        if buffer_chunk:\n",
        "            buffer_chunk += \"\\n\"\n",
        "\n",
        "        buffer_chunk += chunk_text\n",
        "        buffer_length += tiktoken_len(chunk_text)\n",
        "\n",
        "        if buffer_length >= min_tokens:\n",
        "            combined_chunks.append(buffer_chunk)\n",
        "            buffer_chunk = \"\"\n",
        "            buffer_length = 0\n",
        "\n",
        "    # Add any remaining buffer_chunk to the list\n",
        "    if buffer_chunk:\n",
        "        combined_chunks.append(buffer_chunk)\n",
        "\n",
        "    return combined_chunks\n",
        "\n",
        "def split_and_prepend(chunks, max_tokens):\n",
        "  #\"\"\"Split chunks exceeding the max token count and prepend the first line of the original chunk.\"\"\"\n",
        "  final_chunks = []\n",
        "  for chunk in chunks:\n",
        "    if tiktoken_len(chunk) > max_tokens:\n",
        "      first_line = chunk.split(\"\\n\", 1)[0]\n",
        "      split_chunks = text_splitter(chunk, max_tokens)\n",
        "      # Prepend the first line to each split chunk\n",
        "      split_chunks = [first_line + \"\\n\" + sub_chunk for sub_chunk in split_chunks]\n",
        "      final_chunks.extend(split_chunks)\n",
        "    else:\n",
        "      final_chunks.append(chunk)\n",
        "\n",
        "  return final_chunks\n",
        "\n",
        "def get_naive_chunks(user_settings):\n",
        "  chunk_size = user_settings['chunk_size']\n",
        "  #calculating 8% overlap, rounding up to nearest 10 tokens.\n",
        "  chunk_raw = 0.08 * chunk_size\n",
        "  chunk_overlap = math.ceil(chunk_raw / 10) * 10\n",
        "  chunks = text_splitter(user_settings['doc_content'], chunk_size, chunk_overlap)\n",
        "\n",
        "  return chunks\n",
        "\n",
        "def get_contextual_chunks(user_settings):\n",
        "  gradio_doc = user_settings['doc_doc']\n",
        "  file_path = gradio_doc.name\n",
        "  llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
        "  pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
        "  sherpa_doc = pdf_reader.read_pdf(file_path)\n",
        "  combined_chunks = combine_chunks(sherpa_doc.chunks(), user_settings['context_chunk_min'])\n",
        "  return split_and_prepend(combined_chunks, user_settings['context_chunk_max'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNJy2GLQgZX7"
      },
      "source": [
        "##Front End to Back End Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "30KJQxyOgZql"
      },
      "outputs": [],
      "source": [
        "def vectorize(\n",
        "    openai_key, pinecone_key, pinecone_environment, pinecone_index,\n",
        "    naive_namespace, contextual_namespace, doc_doc, doc_content, top_k, chunk_size,\n",
        "    context_chunk_min, context_chunk_max):\n",
        "  #make a dictionary to pass around\n",
        "  user_settings = {\n",
        "      'openai_key': openai_key,\n",
        "      'pinecone_key': pinecone_key,\n",
        "      'pinecone_environment': pinecone_environment,\n",
        "      'pinecone_index': pinecone_index,\n",
        "      'naive_namespace': naive_namespace,\n",
        "      'contextual_namespace': contextual_namespace,\n",
        "      'doc_doc': doc_doc,\n",
        "      'doc_content': doc_content,\n",
        "      'top_k': int(top_k),\n",
        "      'chunk_size': int(chunk_size),\n",
        "      'context_chunk_min': int(context_chunk_min),\n",
        "      'context_chunk_max': int(context_chunk_max)\n",
        "  }\n",
        "  naive_chunks = get_naive_chunks(user_settings)\n",
        "  contextual_chunks = get_contextual_chunks(user_settings)\n",
        "\n",
        "  naive_v_text = upsert_vectors(user_settings, naive_chunks, user_settings['naive_namespace'])\n",
        "  contextual_v_text = upsert_vectors(user_settings, contextual_chunks, user_settings['contextual_namespace'])\n",
        "\n",
        "  return naive_v_text, contextual_v_text\n",
        "\n",
        "def q_and_a(openai_key, pinecone_key, pinecone_environment, pinecone_index, naive_namespace, contextual_namespace,\n",
        "            top_k, question_1=None, question_2=None, question_3=None):\n",
        "  user_settings = {\n",
        "    'openai_key': openai_key,\n",
        "    'pinecone_key': pinecone_key,\n",
        "    'pinecone_environment': pinecone_environment,\n",
        "    'pinecone_index': pinecone_index,\n",
        "    'naive_namespace': naive_namespace,\n",
        "    'contextual_namespace': contextual_namespace,\n",
        "    'top_k': int(top_k),\n",
        "  }\n",
        "      # Initialize default values\n",
        "  naive_answer_1, naive_chunks_1, contextual_answer_1, contextual_chunks_1 = \"\", \"\", \"\", \"\"\n",
        "  naive_answer_2, naive_chunks_2, contextual_answer_2, contextual_chunks_2 = \"\", \"\", \"\", \"\"\n",
        "  naive_answer_3, naive_chunks_3, contextual_answer_3, contextual_chunks_3 = \"\", \"\", \"\", \"\"\n",
        "\n",
        "  # Get values if questions are not None\n",
        "  if question_1:\n",
        "      naive_answer_1, naive_chunks_1 = rag_qa(user_settings, question_1, user_settings['naive_namespace'])\n",
        "      contextual_answer_1, contextual_chunks_1 = rag_qa(user_settings, question_1, user_settings['contextual_namespace'])\n",
        "  if question_2:\n",
        "      naive_answer_2, naive_chunks_2 = rag_qa(user_settings, question_2, user_settings['naive_namespace'])\n",
        "      contextual_answer_2, contextual_chunks_2 = rag_qa(user_settings, question_2, user_settings['contextual_namespace'])\n",
        "  if question_3:\n",
        "      naive_answer_3, naive_chunks_3 = rag_qa(user_settings, question_3, user_settings['naive_namespace'])\n",
        "      contextual_answer_3, contextual_chunks_3 = rag_qa(user_settings, question_3, user_settings['contextual_namespace'])\n",
        "\n",
        "  return (naive_answer_1, contextual_answer_1, naive_chunks_1, contextual_chunks_1,\n",
        "          naive_answer_2, contextual_answer_2, naive_chunks_2, contextual_chunks_2,\n",
        "          naive_answer_3, contextual_answer_3, naive_chunks_3, contextual_chunks_3)\n",
        "\n",
        "  #results = {}\n",
        "  #\n",
        "  #questions = [question_1, question_2, question_3]\n",
        "  #\n",
        "  #for idx, question in enumerate(questions, 1):\n",
        "  #  if question:\n",
        "  #    naive_answer, naive_chunks = rag_qa(user_settings, question, naive_namespace)\n",
        "  #    contextual_answer, contextual_chunks = rag_qa(user_settings, question, contextual_namespace)\n",
        "\n",
        "  #    results[f'naive_answer_{idx}'] = naive_answer\n",
        "  #    results[f'naive_chunks_{idx}'] = naive_chunks\n",
        "  #    results[f'contextual_answer_{idx}'] = contextual_answer\n",
        "  #    results[f'contextual_chunks_{idx}'] = contextual_chunks\n",
        "\n",
        "  #return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz2UOfFuu4sA"
      },
      "source": [
        "## Gradio App UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rUFW3bi9uv4a"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "    with gr.Column(scale=3):\n",
        "      with gr.Tab(\"Vector Chunking\"):\n",
        "        with gr.Accordion(\"API Keys and Pinecone Parameters\"):\n",
        "          gr.Markdown(\n",
        "            \"\"\"\n",
        "            Set up your Keys and Configure Your Vector Store. DO NOT choose existing Pinecone Namespaces, as this App will delete and rewrite the Namespaces if they already exist.\n",
        "            \"\"\")\n",
        "          openai_key = gr.Textbox(scale=1, label=\"OpenAI API Key\", placeholder='sk-...')\n",
        "          pinecone_key = gr.Textbox(scale=1, label=\"Pinecone API Key\", placeholder='...')\n",
        "\n",
        "          with gr.Row():\n",
        "            pinecone_environment = gr.Textbox(lines=1, label=\"Pinecone Environment\", interactive=True, placeholder=\"us-west4-gcp-free\")\n",
        "            pinecone_index = gr.Textbox(lines=1, label=\"Index Name\", interactive=True, placeholder=\"scotus\")\n",
        "          with gr.Row():\n",
        "            naive_namespace = gr.Textbox(lines=1, label=\"Naive Chunking Namespace\", interactive=True, value=\"my_pdf_naive_chunks\")\n",
        "            contextual_namespace = gr.Textbox(lines=1, label=\"Contextual Chunking Namespace\", interactive=True, value=\"my_pdf_contextual_chunks\")\n",
        "\n",
        "        with gr.Row():\n",
        "          input_doc_tokens = gr.Textbox(label=\"Tokens\", scale=1)\n",
        "          top_k = gr.Textbox(label=\"Top_K\", value=3, scale=1)\n",
        "          chunk_size = gr.Slider(100, 1500, value=600, step=50, label=\"Naive Chunk Size\",\n",
        "                                 info=\"Chunk Overlap will automatically be calculated to be approximately 8% of Chunk size\", interactive=True, scale=4)\n",
        "        with gr.Row():\n",
        "          context_chunk_min = gr.Textbox(lines=1, value=\"400\", max_lines=1, label=\"Context Chunk Minimum Size\")\n",
        "          context_chunk_max = gr.Textbox(lines=1, value=\"900\", max_lines=1, label=\"Context Chunk Max Size\")\n",
        "        with gr.Row():\n",
        "          naive_chunk_text = gr.Textbox(lines=15, max_lines=20, label=\"Naive Chunking\", show_copy_button=True)\n",
        "          contextual_chunk_text = gr.Textbox(lines=15, max_lines=20, label=\"Contextually Aware Chunks\", show_copy_button=True)\n",
        "        with gr.Row():\n",
        "          upload_button = gr.UploadButton(\"Upload Doc\", file_types=[\".pdf\"], file_count=\"single\", size=\"sm\")\n",
        "          create_vectors_button = gr.Button(\"Create Vectors\", variant=\"primary\", size=\"sm\")\n",
        "          doc_doc = gr.State()\n",
        "          doc_content = gr.State()\n",
        "\n",
        "# Configure which options to include in Summaries\n",
        "      with gr.Tab(\"Questions and Answers\"):\n",
        "        get_answers_button = gr.Button(\"Submit Questions\", variant=\"primary\", size=\"sm\")\n",
        "        question_1 = gr.Textbox(lines=2, max_lines=4, label=\"Question 1\")\n",
        "        with gr.Accordion(\"Question 1 Chunks and Answers\", open=False):\n",
        "          with gr.Row():\n",
        "            naive_answer_1 = gr.Textbox(lines=15, max_lines=20, label=\"Naive Answer\", show_copy_button=True)\n",
        "            contextual_answer_1 = gr.Textbox(lines=15, max_lines=20, label=\"Contextual Answer\", show_copy_button=True)\n",
        "          with gr.Row():\n",
        "            naive_chunks_1 = gr.Textbox(lines=15, max_lines=20, label=\"Naive Chunks\", show_copy_button=True)\n",
        "            contextual_chunks_1 = gr.Textbox(lines=15, max_lines=20, label=\"Contextual Chunks\", show_copy_button=True)\n",
        "        question_2 = gr.Textbox(lines=2, max_lines=4, label=\"Question 2\")\n",
        "        with gr.Accordion(\"Question 2 Chunks and Answers\", open=False):\n",
        "          with gr.Row():\n",
        "            naive_answer_2 = gr.Textbox(lines=15, max_lines=20, label=\"Naive Answer\", show_copy_button=True)\n",
        "            contextual_answer_2 = gr.Textbox(lines=15, max_lines=20, label=\"Contextual Answer\", show_copy_button=True)\n",
        "          with gr.Row():\n",
        "            naive_chunks_2 = gr.Textbox(lines=15, max_lines=20, label=\"Naive Chunks\", show_copy_button=True)\n",
        "            contextual_chunks_2 = gr.Textbox(lines=15, max_lines=20, label=\"Contextual Chunks\", show_copy_button=True)\n",
        "        question_3 = gr.Textbox(lines=2, max_lines=4, label=\"Question 3\")\n",
        "        with gr.Accordion(\"Question 3 Chunks and Answers\", open=False):\n",
        "          with gr.Row():\n",
        "            naive_answer_3 = gr.Textbox(lines=15, max_lines=20, label=\"Naive Answer\", show_copy_button=True)\n",
        "            contextual_answer_3 = gr.Textbox(lines=15, max_lines=20, label=\"Contextual Answer\", show_copy_button=True)\n",
        "          with gr.Row():\n",
        "            naive_chunks_3 = gr.Textbox(lines=15, max_lines=20, label=\"Naive Chunks\", show_copy_button=True)\n",
        "            contextual_chunks_3 = gr.Textbox(lines=15, max_lines=20, label=\"Contextual Chunks\", show_copy_button=True)\n",
        "\n",
        "  # Pinecone Setup Tab\n",
        "  upload_button.upload(fn=doc_upload, inputs=[upload_button], outputs=[doc_doc, doc_content, input_doc_tokens])\n",
        "  create_vectors_button.click(fn=vectorize, inputs=\n",
        "   [openai_key, pinecone_key, pinecone_environment, pinecone_index,\n",
        "    naive_namespace, contextual_namespace, doc_doc, doc_content, top_k, chunk_size,\n",
        "    context_chunk_min, context_chunk_max], outputs=[naive_chunk_text, contextual_chunk_text]\n",
        "                              )\n",
        "\n",
        "  # Questions and Answers Tab\n",
        "  get_answers_button.click(fn=q_and_a, inputs=[\n",
        "      openai_key, pinecone_key, pinecone_environment, pinecone_index,\n",
        "      naive_namespace, contextual_namespace, top_k, question_1, question_2, question_3],\n",
        "                    outputs=[naive_answer_1, contextual_answer_1, naive_chunks_1, contextual_chunks_1,\n",
        "                             naive_answer_2, contextual_answer_2, naive_chunks_2, contextual_chunks_2,\n",
        "                             naive_answer_3, contextual_answer_3, naive_chunks_3, contextual_chunks_3,\n",
        "                             ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmV2OVobeIHo"
      },
      "source": [
        "#Gradio Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "HHB8DHuoeCPm",
        "outputId": "901b6d79-21c3-4e16-9122-9566e8a827cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://717db58f0f5a4fc983.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://717db58f0f5a4fc983.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://717db58f0f5a4fc983.gradio.live\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo.queue().launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa9x9DXX90GssMHBdO+cd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}